{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF702 Redes Neurais\n",
    "Esse notebook contém um script base para o projeto da disciplina IF702 Redes Neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A leitura do data set é feita utilizando a biblioteca `pandas`. O presente exemplo importa a base de dados `mammography`, assim, caso você esteja trabalhando com outro data set, modifique esta linha.\n",
    "Para importar o conjunto de dados do PAKDD, use a função `pd.read_table` ao invés da `pd.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('data/mammography.csv.zip')\n",
    "data_set.drop_duplicates(inplace=True)  # Remove exemplos repetidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando o data set em atributos dependentes (X = features) e independentes (y = classe). No caso do `mammography` a classe majoritária está codificada como -1 e a classe minoritária está codificada como 1. Para treinar nossa rede neural precisamos que os valores de classe sejam 0 e 1, assim modificamos a codificação da majoritária para 0.\n",
    "\n",
    "Perceba que esse pré-processamento varia de data set para data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_set.iloc[:, :-1].values\n",
    "y = data_set.iloc[:, -1].values\n",
    "y = np.where(y == -1, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui dividimos o data set em treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Treino: 50%, Validação: 25%, Teste: 25%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, \n",
    "                                                    random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/3, \n",
    "                                                  random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o comportamento da rede com diferentes funções de sampling, as mesmas devem ser implementadas e aplicadas ao conjunto de treinamento antes da normalização dos dados (você também pode investigar qual o efeito de aplicar o sampling após a normalização)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TO DO -- Implementar as funções de sampling a serem utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante lembrar de normalizar os dados. A classe `StandardScaler` centraliza as variáveis e transforma as features para terem variância unitária. Você pode testar outras opções como o `MinMaxScaler`.\n",
    "\n",
    "Todas as alternativas estão disponíveis em:\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui definimos a arquitetura de nossa rede neural e treinamos ela.\n",
    "\n",
    "A arquitetura da rede é definida como tendo apenas uma camada escondida. O código é bem intuitivo e a adição de novas camadas pode ser feita através da função `add`.\n",
    "\n",
    "Várias funções de otimização estão disponíveis e seus parâmetros também podem ser definidos. \n",
    "\n",
    "Confira os exemplos em: https://keras.io/optimizers/\n",
    "\n",
    "Um maior controle sobre quando a rede deve parar de treinar também pode ser exercido. \n",
    "\n",
    "Confira a documentação da classe `EarlyStopping`: https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o esboço da rede.\n",
    "classifier = Sequential()\n",
    "# Adiciona a primeira camada escondida contendo 16 neurônios e função de ativação tangente \n",
    "# hiperbólica. Por ser a primeira camada adicionada à rede, precisamos especificar a \n",
    "# dimensão de entrada (número de features do data set), no caso do mammography são 6.\n",
    "classifier.add(Dense(16, activation='tanh', input_dim=6))\n",
    "# Adiciona a camada de saída. Como nosso problema é binário, só precisamos de 1 neurônio \n",
    "# e função de ativação sigmoidal. A partir da segunda camada adicionada, keras já consegue \n",
    "# inferir o número de neurônios de entrada (nesse caso 16) e nós não precisamos mais \n",
    "# especificar.\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "# Compila o modelo especificando o otimizador, a função de custo, e opcionalmente métricas \n",
    "# para serem observadas durante o treinamento.\n",
    "classifier.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Treina a rede, especificando o tamanho do batch, o número máximo de épocas, se deseja \n",
    "# parar prematuramente caso o erro de validação não decresça, e o conjunto de validação.\n",
    "history = classifier.fit(X_train, y_train, batch_size=64, epochs=100000, \n",
    "                         callbacks=[EarlyStopping()], validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição de funções auxiliares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_final_losses(history):\n",
    "    \"\"\"Função para extrair o loss final de treino e validação.\n",
    "    \n",
    "    Argumento(s):\n",
    "    history -- Objeto retornado pela função fit do keras.\n",
    "    \n",
    "    Retorno:\n",
    "    Dicionário contendo o loss final de treino e de validação.\n",
    "    \"\"\"\n",
    "    return {'train_loss': history.history['loss'][-1], 'val_loss': history.history['val_loss'][-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar a nossa rede para fazer predições e computar métricas de desempenho.\n",
    "\n",
    "Mais métricas de desempenho: http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fazer predições no conjunto de teste\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred_class = classifier.predict_classes(X_test, verbose=0)\n",
    "\n",
    "## Matriz de confusão\n",
    "print('Matriz de confusão')\n",
    "print(confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "## Computar métricas de desempenho\n",
    "losses = extract_final_losses(history)\n",
    "print(\"\\n{metric:<18}{value:.4f}\".format(metric=\"Train Loss:\", value=losses['train_loss']))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Validation Loss:\", value=losses['val_loss']))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Accuracy:\", value=accuracy_score(y_test, y_pred_class)))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Recall:\", value=recall_score(y_test, y_pred_class)))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"Precision:\", value=precision_score(y_test, y_pred_class)))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"F1:\", value=f1_score(y_test, y_pred_class)))\n",
    "print(\"{metric:<18}{value:.4f}\".format(metric=\"AUROC:\", value=roc_auc_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
